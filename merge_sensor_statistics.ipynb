{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis scripts merges the statistics of 2 sensors into 1\\nIt does this for multiple sensors because i had added almost the same sensors via 2 different integrations\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This scripts merges the statistics of 2 sensors into 1\n",
    "It does this for multiple sensors because i had added almost the same sensors via 2 different integrations\n",
    "\n",
    "Script can also be used to recover statistics from corrupted database\n",
    "\n",
    "\n",
    "Before executing the script, stop you homeassistant service:\n",
    "ssh jan@192.168.1.25\n",
    "sudo systemctl stop home-assistant@homeassistant.service \n",
    "\n",
    "Don't forget to start afterwards:\n",
    "sudo systemctl start home-assistant@homeassistant.service \n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters to change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "hass_ip=\"192.168.1.25\" # change this to the ip of you home assistant installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping of the sensors, statistics of the left ones will be merged into the right ones\n",
    "mapping={'sensor.nibe_93714_44270': 'sensor.calc_cooling_supply_s1_44270',\n",
    "        'sensor.nibe_93714_40013': 'sensor.bt7_hw_top_40013',\n",
    "        'sensor.nibe_93714_40015': 'sensor.eb100_ep14_bt10_brine_in_temp_40015',\n",
    "        'sensor.nibe_93714_40004': 'sensor.bt1_outdoor_temperature_40004',\n",
    "        'sensor.nibe_93714_40012': 'sensor.eb100_ep14_bt3_return_temp_40012',\n",
    "        'sensor.nibe_93714_40018': 'sensor.eb100_ep14_bt14_hot_gas_temp_40018',\n",
    "        'sensor.nibe_93714_40017': 'sensor.eb100_ep14_bt12_condensor_out_40017',\n",
    "        'sensor.nibe_93714_40033': 'sensor.bt50_room_temp_s1_40033',\n",
    "        'sensor.nibe_93714_40067': 'sensor.bt1_average_40067',\n",
    "        'sensor.nibe_93714_40016': 'sensor.eb100_ep14_bt11_brine_out_temp_40016',\n",
    "        'sensor.nibe_93714_40014': 'sensor.bt6_hw_load_40014',\n",
    "        'sensor.nibe_93714_43009': 'sensor.calc_supply_s1_43009',\n",
    "        'sensor.nibe_93714_40019': 'sensor.eb100_ep14_bt15_liquid_line_40019',\n",
    "        'sensor.nibe_93714_40008': 'sensor.bt2_supply_temp_s1_40008',\n",
    "        'sensor.nibe_93714_40022': 'sensor.eb100_ep14_bt17_suction_40022'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_db_path=\"home-assistant_v2.db\"\n",
    "\n",
    "#  in case you want to merge two sensors in the same db:\n",
    "source_db_path = target_db_path \n",
    "# otherwise if there is really a seperate db used as source:\n",
    "# source_db_path = \"home-assistant_v2.db.corrupt.2023-03-12T03:12:13.457493+00:00\" \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve database from home assistant server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "home-assistant_v2.db                          100%   97MB  11.6MB/s   00:08    \n"
     ]
    }
   ],
   "source": [
    "# add your public key to the homeassistant account to make scp possible\n",
    "! scp homeassistant@{hass_ip}:/home/homeassistant/.homeassistant/home-assistant_v2.db .\n",
    "! mkdir -p backup\n",
    "! cp home-assistant_v2.db backup # make a backup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import sqlite3\n",
    "import pandas\n",
    "pandas.options.plotting.backend = \"plotly\"\n",
    "\n",
    "con_target = sqlite3.connect(target_db_path)\n",
    "if source_db_path != target_db_path :\n",
    "    con_source = sqlite3.connect(source_db_path)\n",
    "else:\n",
    "    con_source = con_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case you want to merge all sensors from source_db into target_db\n",
    "# mapping = {sens:sens for sens in pandas.read_sql(\"\"\"select statistic_id from statistics_meta\"\"\", con_target)['statistic_id'].values}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(source_sensor, target_sensor, table='statistics'):\n",
    "    global source_df, target_df\n",
    "    print(f\"source: {source_sensor}, target: {target_sensor}\")\n",
    "    # read data from target sensor\n",
    "    target_sensor_id, has_sum=pandas.read_sql_query(f\"\"\"select id, has_sum FROM statistics_meta \n",
    "                                                where statistic_id like '{target_sensor}';\"\"\", con_target).loc[0,['id','has_sum']]\n",
    "    target_df=pandas.read_sql_query(\n",
    "            f\"select * FROM {table} where metadata_id = '{target_sensor_id}';\", con_target\n",
    "        )\n",
    "    print(f\"length of existing statistics for target sensor: {len(target_df)}\")\n",
    "\n",
    "    min_dt = target_df[\"start\"].min()\n",
    "\n",
    "    # read data from source sensor\n",
    "    source_sensor_id=pandas.read_sql_query(f\"\"\"select id FROM statistics_meta \n",
    "                                                where statistic_id like '{source_sensor}';\"\"\", con_source).loc[0,'id']\n",
    "    source_df=pandas.read_sql_query(\n",
    "            f\"select * FROM {table} where metadata_id = '{source_sensor_id}' and start < '{min_dt}';\", con_source\n",
    "        )\n",
    "    print(f\"length of existing statistics for source sensor: {len(source_df)}\")\n",
    "\n",
    "    # or only look at sum and assume first record of target is same as last record of source\n",
    "    if has_sum:\n",
    "        # source_offset=source_df['sum'].max() #last record should be equal \n",
    "        source_offset=source_df.loc[source_df['start']==source_df['start'].max(),\"sum\"].max()\n",
    "        source_df['sum']=source_df['sum']-source_offset       \n",
    "\n",
    "    # concat new historical statistics with updated old statistics\n",
    "    df = pandas.concat([source_df, target_df], ignore_index=True).sort_values(\"start\")\n",
    "\n",
    "    # set metadata_id to the one of the target sensor\n",
    "    df[\"metadata_id\"]=target_sensor_id\n",
    "\n",
    "    # deduplicate timestamp in case of non-clean merge\n",
    "    df=df.drop_duplicates(subset=['start'], keep='first')\n",
    "\n",
    "    # set a temporary unique id\n",
    "    max_id=pandas.read_sql_query(f\"\"\"select max(id) FROM {table};\"\"\", con_target).loc[0,'max(id)']\n",
    "    df['id']=range(max_id+1,max_id+len(df)+1)\n",
    "\n",
    "    # drop all existing rows with same metadata_id as the sensors\n",
    "    stmnt = f\"\"\"DELETE FROM {table}\n",
    "                WHERE metadata_id = {target_sensor_id};\"\"\"\n",
    "    cur = con_target.cursor()\n",
    "    cur.execute(stmnt)\n",
    "    con_target.commit()\n",
    "\n",
    "    # drop all existing rows with same metadata_id as the sensors\n",
    "    stmnt = f\"\"\"DELETE FROM {table}\n",
    "                WHERE metadata_id = {source_sensor_id};\"\"\"\n",
    "    cur = con_source.cursor()\n",
    "    cur.execute(stmnt)\n",
    "    con_source.commit()\n",
    "\n",
    "    # insert new data into table of target db\n",
    "    df.to_sql(\n",
    "        table, con_target, schema=None, if_exists=\"append\", index=False,\n",
    "    )\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "for source_sensor, target_sensor in mapping.items():\n",
    "    for table in (\"statistics\", \"statistics_short_term\"):    \n",
    "        print(table)\n",
    "        test=merge(source_sensor, target_sensor, table)\n",
    "        # fig=test[['state','sum']].plot()\n",
    "        # fig=test[['mean']].plot()\n",
    "        # fig.show()\n",
    "        # input(\"Press to continue\")\n",
    "        # clear_output(wait=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reindex entire table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for table in (\"statistics\", \"statistics_short_term\"):   \n",
    "    df=pandas.read_sql_query(\n",
    "        f\"select * FROM {table};\", con_target\n",
    "    )\n",
    "    \n",
    "    # reindex\n",
    "    df['id']=range(1, len(df) + 1)\n",
    "\n",
    "    # drop all existing rows with same metadata_id as the sensors\n",
    "    stmnt = f\"\"\"DELETE FROM {table};\"\"\"\n",
    "    cur = con_target.cursor()\n",
    "    cur.execute(stmnt)\n",
    "    con_target.commit()\n",
    "\n",
    "    # insert new data into table\n",
    "    df.to_sql(\n",
    "        table, con_target, schema=None, if_exists=\"append\", index=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_target.close()\n",
    "con_source.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy db back to home assistant server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "home-assistant_v2.db                          100%  276MB  11.9MB/s   00:23    \n"
     ]
    }
   ],
   "source": [
    "! scp home-assistant_v2.db homeassistant@{hass_ip}:/home/homeassistant/.homeassistant/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "watermeter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "29f62ddf6052fc3cbbf17655180ba76afb223cb4671e29d3f02f8ca2f3b5d212"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
